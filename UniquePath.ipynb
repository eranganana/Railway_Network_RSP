{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b95e7789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9bfe0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tåguppdrag', 'Datum_PAU', 'Tågslag', 'Tågsort', 'UppehållstypAvgång', 'UppehållstypAnkomst', 'AktivitetskodAvgång', 'AktivitetskodBeskrivningAvgång', 'AktivitetskodAnkomst', 'AktivitetskodBeskrivningAnkomst', 'DelSträckanummer', 'Förstaplatssignatur_för_Uppdrag', 'Sistaplatssignatur_för_Uppdrag', 'Avgångsplats', 'Avgångsplatssignatur', 'Ankomstplats', 'Ankomstplatssignatur', 'PlanAvgTid', 'PlanAnkTid', 'UtfAnkTid', 'UtfAvgTid', 'PlanAnkTid_vid_AvgPlats', 'UtfAnkTid_vid_AvgPlats', 'PlanUppehållstidAvgång', 'UtfUppehållstidAvgång', 'PlanGångtid', 'UtfGångtid', 'FörseningGångtid', 'AvgFörsening', 'AnkFörsening', 'FörseningUppehållAvgång']\n"
     ]
    }
   ],
   "source": [
    "sample_data = pd.read_csv('../../data/month_data.csv', delimiter=';',encoding='utf-8')  # or 'utf-8'\n",
    "print(sample_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "159853b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Förstaplatssignatur_för_Uppdrag Sistaplatssignatur_för_Uppdrag  Count\n",
      "258                              Cst                           Arnn  73352\n",
      "26                              Arnn                            Cst  73321\n",
      "269                              Cst                            Hgl  55075\n",
      "747                              Hgl                            Cst  53556\n",
      "514                                G                             Än  41475\n",
      "...                              ...                            ...    ...\n",
      "1479                              My                             Gi      1\n",
      "809                               Hm                           Hbgb      1\n",
      "540                               Gk                            Krd      1\n",
      "1443                              Mr                             Rs      1\n",
      "1453                             Mra                            Khn      1\n",
      "\n",
      "[2442 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Get the true start stations (where UppehållstypAvgång == \"Första\")\n",
    "start_stations = sample_data[sample_data['UppehållstypAvgång'] == 'Första'][['Tåguppdrag', 'Förstaplatssignatur_för_Uppdrag', 'Datum_PAU']]\n",
    "\n",
    "# Get the true end stations (where UppehållstypAnkomst == \"Sista\")\n",
    "end_stations = sample_data[sample_data['UppehållstypAnkomst'] == 'Sista'][['Tåguppdrag', 'Sistaplatssignatur_för_Uppdrag']]\n",
    "\n",
    "# Combine them (this assumes each journey has exactly one start and one end record)\n",
    "journeys = pd.merge(start_stations, end_stations, on='Tåguppdrag')\n",
    "\n",
    "# Count journeys between station pairs\n",
    "journey_counts = journeys.groupby(['Förstaplatssignatur_för_Uppdrag', 'Sistaplatssignatur_för_Uppdrag']).size().reset_index(name='Count')\n",
    "journey_counts = journey_counts.sort_values('Count', ascending=False)\n",
    "\n",
    "print(journey_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62ccd3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get full journey path\n",
    "# Get all stations visited by each train in order\n",
    "train_paths = (\n",
    "    sample_data.sort_values(['Tåguppdrag', 'DelSträckanummer'])\n",
    "    .groupby('Tåguppdrag')['Ankomstplatssignatur']\n",
    "    .apply(lambda x: tuple(x))\n",
    "    .reset_index(name='Path')\n",
    ")\n",
    "\n",
    "# Add start and end stations\n",
    "train_paths['Start'] = train_paths['Path'].str[0]\n",
    "train_paths['End'] = train_paths['Path'].str[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cdcb7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Start   End                                               Path  Train_Count\n",
      "0     A     F                      (A, Agg, Vgå, Hr, Kä, Fby, F)            1\n",
      "1     A   Sär  (A, Bgs, Vbd, Ns, Ndv, Fd, Sn, Lr, Asd, Apn, J...            1\n",
      "2   Aal  Hbgb                 (Aal, Hlp, Ka, Öda, Mia, Hb, Hbgb)            1\n",
      "3   Aal   Mia                           (Aal, Hlp, Ka, Öda, Mia)            1\n",
      "4   Acm  Avky  (Acm, Acm, Acm, Acm, Acm, Skä, Skä, Skä, Skä, ...            1\n",
      "5   Acm   Blg  (Acm, Acm, Acm, Acm, Acm, Acm, Acm, Acm, Acm, ...            1\n",
      "6   Acm   Blg  (Acm, Acm, Acm, Acm, Snb, Snb, Snb, Snb, Hdm, ...            1\n",
      "7   Acm   Blg  (Acm, Ju, Avky, Snb, Hdm, Vhy, St, Gtf, Sau, Blg)            1\n",
      "8   Acm   Blg            (Acm, Snb, Hdm, Vhy, St, Gtf, Sau, Blg)            1\n",
      "9   Acm   Gså  (Acm, Acm, Acm, Snb, Snb, Snb, Hdm, Hdm, Vhy, ...            1\n"
     ]
    }
   ],
   "source": [
    "#find unique routes\n",
    "# Count how many trains take each unique path between start and end stations\n",
    "path_counts = (\n",
    "    train_paths.groupby(['Start', 'End', 'Path'])\n",
    "    .size()\n",
    "    .reset_index(name='Train_Count')\n",
    "    .sort_values(['Start', 'End', 'Train_Count'], ascending=[True, True, False])\n",
    ")\n",
    "\n",
    "print(path_counts.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20bd31f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Start   End  Unique_Paths  \\\n",
      "0     Acm  Avky            13   \n",
      "1     Acm   Blg             4   \n",
      "2     Ahm     N             5   \n",
      "3     Ahm    Sy             6   \n",
      "4      Al    Fv            12   \n",
      "..    ...   ...           ...   \n",
      "558   Öte  Brny             3   \n",
      "559   Övn    Cr            34   \n",
      "560   Övn  Hrbg             3   \n",
      "561   Övn   Mgb            14   \n",
      "562   Övn   Trg            14   \n",
      "\n",
      "                                   Path_Variations_Str  \n",
      "0    [Acm → Pöb → Gk → Snb → Hdm → Or → Vhy → Gsv →...  \n",
      "1    [Acm → Ju → Snb → Avky → Hdm → Vhy → St → Gtf ...  \n",
      "2    [Ahm → Gt → Sä → Bdf → Utp → Utp → Bdf → Sä → ...  \n",
      "3    [Ahm → Öb → Ms → Sä → Utp → Kla → Hpbg → Bdf →...  \n",
      "4    [Al → Feb → Ålg → Blv → Jbk → Åk → Arb → Åkn →...  \n",
      "..                                                 ...  \n",
      "558  [Öte → Dån → Rön → Gau → Södy → Tu → Uts → Tul...  \n",
      "559  [Övn → Tri → M → Rog → Mpb → Mgb → Al → Fsb → ...  \n",
      "560  [Övn → Svö → Rog → Ög → Vid → Mpb → Lrp → Fsb ...  \n",
      "561  [Övn → Svö → Rog → Ög → Mpb → Vid → Fsb → Lrp ...  \n",
      "562  [Övn → Svö → Rog → Ög → Vid → Mpb → Lrp → Fsb ...  \n",
      "\n",
      "[560 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Get unique paths per train per day\n",
    "unique_daily_paths = (\n",
    "    sample_data.sort_values(['Tåguppdrag', 'Datum_PAU', 'DelSträckanummer'])\n",
    "    .groupby(['Tåguppdrag', 'Datum_PAU'])['Ankomstplatssignatur']\n",
    "    .apply(lambda x: tuple(x))\n",
    "    .reset_index(name='Path')\n",
    "    .drop_duplicates(['Tåguppdrag', 'Path'])  # Remove duplicate paths for same train\n",
    ")\n",
    "\n",
    "# Step 2: Add start and end stations\n",
    "unique_daily_paths['Start'] = unique_daily_paths['Path'].apply(lambda x: x[0])\n",
    "unique_daily_paths['End'] = unique_daily_paths['Path'].apply(lambda x: x[-1])\n",
    "\n",
    "# Step 3: Count distinct paths per OD pair\n",
    "od_path_counts = (\n",
    "    unique_daily_paths.groupby(['Start', 'End'])\n",
    "    .agg(Unique_Paths=('Path', 'nunique'))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Step 4: Filter for OD pairs with >2 path variations\n",
    "multi_path_routes = od_path_counts[od_path_counts['Unique_Paths'] > 2]\n",
    "\n",
    "# Step 5: Get example paths for these OD pairs\n",
    "example_paths = (\n",
    "    unique_daily_paths[unique_daily_paths.set_index(['Start', 'End']).index.isin(\n",
    "        multi_path_routes.set_index(['Start', 'End']).index)]\n",
    "    .groupby(['Start', 'End'])['Path']\n",
    "    .unique()\n",
    "    .reset_index(name='Path_Variations')\n",
    ")\n",
    "\n",
    "# Merge with the counts\n",
    "result = pd.merge(\n",
    "    multi_path_routes,\n",
    "    example_paths,\n",
    "    on=['Start', 'End'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Convert paths to readable strings and filter out single-station paths\n",
    "result['Path_Variations_Str'] = result['Path_Variations'].apply(\n",
    "    lambda x: [' → '.join(p) for p in x if len(p) > 1]  # Remove single-station paths\n",
    ")\n",
    "\n",
    "# Filter out OD pairs that ended up with ≤2 paths after cleaning\n",
    "result = result[result['Path_Variations_Str'].apply(len) > 2]\n",
    "\n",
    "# Final output\n",
    "print(result[['Start', 'End', 'Unique_Paths', 'Path_Variations_Str']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56b6ed14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 unique paths between Acm and Blg:\n",
      "Path 1: Acm → Ju → Snb → Avky → Hdm → Vhy → St → Gtf → Sau → Blg\n",
      "Path 2: Acm → Ju → Avky → Snb → Hdm → Vhy → St → Gtf → Sau → Blg\n",
      "Path 3: Acm → Snb → Hdm → Vhy → St → Gtf → Sau → Blg\n",
      "Path 4: Acm → Sau → Ju → Hma → Hma → Avky → Avky → Snb → Gtf → Kbn → St → Hdm → Vhy → Vhy → St → Hdm → Snb → Gtf → Acm → Sau → Avky → Blg\n",
      "\n",
      "Saved full details of 268 records to 'acm_to_blg_all_paths_details.csv'\n",
      "Contains 7 unique trains following these routes\n"
     ]
    }
   ],
   "source": [
    "# First get all unique paths between Acm and Blg\n",
    "acm_blg_paths = (\n",
    "    sample_data.sort_values(['Tåguppdrag', 'Datum_PAU', 'DelSträckanummer'])\n",
    "    .groupby(['Tåguppdrag', 'Datum_PAU'])['Ankomstplatssignatur']\n",
    "    .apply(lambda x: tuple(x))\n",
    "    .reset_index(name='Path')\n",
    "    .assign(Start=lambda x: x['Path'].apply(lambda p: p[0]),\n",
    "            End=lambda x: x['Path'].apply(lambda p: p[-1]))\n",
    "    .query(\"Start == 'Acm' and End == 'Blg'\")\n",
    ")\n",
    "\n",
    "# Get the unique path variations\n",
    "unique_paths = acm_blg_paths['Path'].unique()\n",
    "\n",
    "print(f\"Found {len(unique_paths)} unique paths between Acm and Blg:\")\n",
    "for i, path in enumerate(unique_paths, 1):\n",
    "    print(f\"Path {i}: {' → '.join(path)}\")\n",
    "\n",
    "# Get all train numbers that follow these paths\n",
    "relevant_trains = acm_blg_paths['Tåguppdrag'].unique()\n",
    "\n",
    "# Filter original dataset for these trains and routes\n",
    "acm_blg_full_details = (\n",
    "    sample_data[sample_data['Tåguppdrag'].isin(relevant_trains)]\n",
    "    .sort_values(['Tåguppdrag', 'Datum_PAU', 'DelSträckanummer'])\n",
    "    .assign(Start=lambda x: x.groupby(['Tåguppdrag', 'Datum_PAU'])['Ankomstplatssignatur'].transform('first'),\n",
    "            End=lambda x: x.groupby(['Tåguppdrag', 'Datum_PAU'])['Ankomstplatssignatur'].transform('last'))\n",
    "    .query(\"Start == 'Acm' and End == 'Blg'\")\n",
    ")\n",
    "\n",
    "# Add path identification\n",
    "acm_blg_full_details['Path'] = (\n",
    "    acm_blg_full_details.groupby(['Tåguppdrag', 'Datum_PAU'])['Ankomstplatssignatur']\n",
    "    .transform(lambda x: tuple(x))\n",
    ")\n",
    "\n",
    "# Save to CSV\n",
    "acm_blg_full_details.to_csv('acm_to_blg_all_paths_details.csv', index=False)\n",
    "\n",
    "print(f\"\\nSaved full details of {len(acm_blg_full_details)} records to 'acm_to_blg_all_paths_details.csv'\")\n",
    "print(f\"Contains {len(relevant_trains)} unique trains following these routes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a9484ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average delay per path\n",
    "path_avg_delay = (\n",
    "    acm_blg_full_details.groupby('Path')['AvgFörsening']\n",
    "    .mean()\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Compute average delay per OD pair\n",
    "od_avg_delay = (\n",
    "    acm_blg_full_details.groupby(['Start', 'End'])['AvgFörsening']\n",
    "    .mean()\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Global average delay\n",
    "global_avg_delay = acm_blg_full_details['AvgFörsening'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bdec399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_avg_forsening(row):\n",
    "    if not pd.isna(row['AvgFörsening']):\n",
    "        return row['AvgFörsening']\n",
    "    \n",
    "    path_key = row['Path']\n",
    "    od_key = (row['Start'], row['End'])\n",
    "\n",
    "    if path_key in path_avg_delay:\n",
    "        return path_avg_delay[path_key]\n",
    "    elif od_key in od_avg_delay:\n",
    "        return od_avg_delay[od_key]\n",
    "    else:\n",
    "        return global_avg_delay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d60fc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned dataset with imputed delays to 'acm_to_blg_corrected_delays.csv'\n"
     ]
    }
   ],
   "source": [
    "#apply the imputation\n",
    "acm_blg_full_details['AvgFörsening_Imputed'] = acm_blg_full_details.apply(impute_avg_forsening, axis=1)\n",
    "\n",
    "# Save the cleaned dataset with imputed delays\n",
    "acm_blg_full_details.to_csv('acm_to_blg_corrected_delays.csv', index=False)\n",
    "\n",
    "print(\"Saved cleaned dataset with imputed delays to 'acm_to_blg_corrected_delays.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
